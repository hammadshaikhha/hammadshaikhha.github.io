---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---



Research 
---
**Improving Online Learning Through Course Design: A Microeconomic Approach (2021)**
> Abstract and draft coming soon. 

**Provision of Online Public Goods: Evidence From a Peer Discussion Board (2020)**
> Abstract and draft coming soon. 

**The Impact of Information Disclosure on Student Course Selection (2018)**, with Robert McMillan and Linda Wang
> University students may enrol in less challenging courses despite being eligible for more rigorous courses that may be more beneficial for them. This mismatch of easy courses and high-quality students discourages departments from offering challenging courses, and also limits the human capital potential of post-secondary students. We conduct a randomized experiment to examine the impact of informing high-achieving first-year economics students about their future course options. We find that providing an information session significantly increases the probability of eligible first-year students enrolling in the most rigorous second-year economics courses. Our result sheds light on the determinants of students’ course selection and suggests course enrolment strategies for departments and universities.

**Revisiting the Racial Achievement Gap: Evidence from Recent US Panel Data (2017)**
> This paper examines the racial gap in academic achievement in elementary and high school using recently released large-scale nationally representative panel data sets from the US. I find achievement disparities among white, black, Asian, and Hispanic students at kindergarten entry in mathematics, reading, science and working memory. Controlling for differences in household and school characteristics across race explains about 70% of the raw minority-white gaps, but moderate unexplained gaps still remain. Black-white and Hispanic-white math and reading achievement gaps widen as children progress through school, but achievement gaps in science and working memory remain constant. Significant racial achievement gaps exist in all key subjects in grade 9 and remain constant throughout high school. Bond and Lang (2013) argue that test scores are generally ordinal and show that certain results in the racial achievement gap literature depend on the cardinalization of the test score scale. To check the sensitivity of my primary results I use the methodology developed by Penney (2017) that provide estimates robust to the ordinality critique.

---



Other Research on Adaptive Experiments in Online Education
---

**[Challenges in Statistical Analysis of Data Collected by a Bandit Algorithm: An Empirical Exploration in Applications to Adaptively Randomized Experiments (2021)](https://arxiv.org/pdf/2103.12198.pdf)**. Joseph Jay Williams, Jacob Nogas, Nina Deliu, Hammad Shaikh, Sofia Villar, Audrey Durand, and Anna Rafferty
>Multi-armed bandit algorithms have been argued for decades as useful for adaptively randomized experiments. In such experiments, an algorithm varies which arms (e.g. alternative interventions to help students learn) are assigned to participants, with the goal of assigning higher-reward arms to as many participants as possible. We applied the bandit algorithm Thompson Sampling (TS) to run adaptive experiments in three university classes. Instructors saw great value in trying to rapidly use data to give their students in the experiments better arms (e.g. better explanations of a concept). Our deployment, however, illustrated a major barrier for scientists and practitioners to use such adaptive experiments: a lack of quantifiable insight into how much statistical analysis of specific real-world experiments is impacted (Pallmann et al, 2018; FDA, 2019), compared to traditional uniform random assignment. We therefore use our case study of the ubiquitous two-arm binary reward setting to empirically investigate the impact of using Thompson Sampling instead of uniform random assignment. In this setting, using common statistical hypothesis tests, we show that collecting data with TS can as much as double the False Positive Rate (FPR; incorrectly reporting differences when none exist) and the False Negative Rate (FNR; failing to report differences when they exist). We empirically illustrate how and why this
occurs–maximizing rewards can lead to biased estimates of arm means (which increases false positives),as well as reduced confidence in estimates of means (which increases false negatives). We find that even when two arms have equal reward, TS can misleadingly allocate many participants to one arm. We show this problem persists using different statistical tests and a Bayesian analysis of the data. We show how two methods for incorporating knowledge of the bandit algorithm into the statistical test can help, but do not eliminate the issues in drawing inferences from adaptive data collection. These empirical results illustrate the nature of the problems to be solved, for more widespread application of bandit algorithms to adaptive real-world experiments, by incorporating statistical considerations into applying, developing, and evaluating bandit algorithms.


**[Balancing Student Success and Inferring Personalized Effects in Dynamic Experiments (2019)](https://drive.google.com/file/d/1C_KGCl0PrEuX8dIv9UWszOt9IR176i-M/view)**. _Proceedings of the 12th International Conference on Educational Data Mining_. Hammad Shaikh, Arghavan Modiri, Joseph Jay Williams, and Anna Rafferty.  [[Poster](https://drive.google.com/open?id=1aKbzUiUlwCR512oVU6yma-qUjfYpzCAE)] [[Slides](https://drive.google.com/open?id=1CsVCXT2VMx6OWtyNLo1Y5KTloQ5YJ1nS)]
> Randomized controlled trials (RCTs) can be embedded in educational technologies to evaluate how interventions affect student outcomes and how effectiveness varies with characteristics like prior knowledge. But RCTs often assign many students to ineffective conditions. Adaptive algorithms like contextual multi-armed bandits (MABs) could change how students are assigned to conditions over time, offering the potential to both evaluate effectiveness for subgroups of students and direct more students to interventions that are effective for them. We use simulations to compare contextual MABs to traditional RCTs and non-contextual MABs. Contextual MABs improve student outcomes for each subgroup; in contrast, non-contextual MABs may help one group of students, such as those with high prior knowledge, while hurting another. Because both MAB algorithms adaptively assign conditions based on prior students’ results, both recover biased estimates of condition effectiveness. However data collected from a contextual MAB is still nearly as good for inferring the optimal policy assignment as a traditional RCT.

---


